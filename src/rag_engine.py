import os
import shutil
import time

# --- C·∫§U H√åNH GOOGLE CHAT (REST) ---
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["GLOG_minloglevel"] = "2"
import google.generativeai as genai
if os.getenv("GOOGLE_API_KEY"):
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"), transport="rest")

from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings

class EnterpriseRAG:
    def __init__(self, persist_directory="./chroma_db"):
        self.persist_directory = persist_directory
        self.vector_store = None
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")
        
        # HuggingFace API
        if self.hf_token:
            self.embedding_model = HuggingFaceInferenceAPIEmbeddings(
                api_key=self.hf_token,
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
        else:
            self.embedding_model = None

    def index_knowledge_base(self):
        if not self.hf_token: return "‚ùå L·ªói: Thi·∫øu HUGGINGFACEHUB_API_TOKEN."

        # 1. D·ªçn d·∫πp DB c≈©
        if os.path.exists(self.persist_directory):
            try: shutil.rmtree(self.persist_directory)
            except: pass

        if not os.path.exists("data"):
            os.makedirs("data")
            return "Folder data created."
            
        all_documents = []
        print("--- üöÄ START INDEXING V4 (ANTI-COLD-START) ---")
        
        # 2. Qu√©t t√†i li·ªáu
        for root, dirs, files in os.walk("data"):
            category = os.path.basename(root) if root != "data" else "General"
            docs = []
            try: docs.extend(DirectoryLoader(root, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'}, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.pdf", loader_cls=PyPDFLoader, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.docx", loader_cls=Docx2txtLoader, silent_errors=True).load())
            except: pass
            
            for doc in docs: 
                doc.metadata["category"] = category
                doc.metadata["source_name"] = os.path.basename(doc.metadata.get("source", ""))
            
            all_documents.extend(docs)

        if not all_documents: return "No documents found."
        
        # 3. C·∫Øt nh·ªè vƒÉn b·∫£n
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(all_documents)
        print(f"T·ªïng: {len(texts)} ƒëo·∫°n vƒÉn.")

        # 4. L∆∞u v√†o DB (Batch nh·ªè + Retry m·∫°nh m·∫Ω)
        try:
            self.vector_store = Chroma(
                embedding_function=self.embedding_model,
                persist_directory=self.persist_directory
            )
            
            batch_size = 5  # Gi·∫£m xu·ªëng 5 ƒë·ªÉ c·ª±c k·ª≥ nh·∫π
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i : i + batch_size]
                current_batch_num = i//batch_size + 1
                
                # C∆† CH·∫æ TH·ª¨ L·∫†I 5 L·∫¶N (ƒê·ªÉ ch·ªù Model th·ª©c d·∫≠y)
                max_retries = 5
                success = False
                
                for attempt in range(max_retries):
                    try:
                        self.vector_store.add_documents(batch)
                        success = True
                        print(f"‚úÖ ƒê√£ xong l√¥ {current_batch_num}/{total_batches}")
                        time.sleep(0.5) 
                        break 
                    except Exception as e:
                        # B·∫Øt l·ªói KeyError (d·∫•u hi·ªáu model ƒëang ng·ªß)
                        err_msg = str(e)
                        if "KeyError" in type(e).__name__ or "'0'" in err_msg:
                            print(f"‚ö†Ô∏è Model ƒëang ng·ªß... ƒê·ª£i 5s ƒë·ªÉ g·ªçi d·∫≠y (L·∫ßn {attempt+1})")
                            time.sleep(5) # Ng·ªß l√¢u h∆°n ƒë·ªÉ ch·ªù model load
                        else:
                            print(f"‚ö†Ô∏è L·ªói m·∫°ng l√¥ {current_batch_num}: {err_msg}. Th·ª≠ l·∫°i...")
                            time.sleep(2)
                
                if not success:
                    return f"‚ùå Th·∫•t b·∫°i t·∫°i l√¥ {current_batch_num}. HuggingFace ƒëang qu√° t·∫£i."
                
            return f"‚úÖ (V4) Th√†nh c√¥ng! ƒê√£ h·ªçc xong {len(all_documents)} t√†i li·ªáu."
            
        except Exception as e:
            return f"‚ùå L·ªói Indexing V4: {str(e)}"

    def retrieve_answer(self, query, chat_history="", category=None):
        if not self.api_key: return "L·ªói: Ch∆∞a c·∫•u h√¨nh Google API Key."
        if not self.embedding_model: return "L·ªói: Ch∆∞a c·∫•u h√¨nh HuggingFace Token."
            
        self.vector_store = Chroma(
            persist_directory=self.persist_directory, 
            embedding_function=self.embedding_model
        )
        
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", 
            google_api_key=self.api_key, 
            temperature=0.1,
            transport="rest"
        )
        
        search_kwargs = {"k": 5}
        if category: search_kwargs["filter"] = {"category": category}

        try:
            retriever = self.vector_store.as_retriever(search_kwargs=search_kwargs)
            relevant_docs = retriever.invoke(query)
            
            if not relevant_docs:
                return "H·ªá th·ªëng ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng ch·∫°y Re-index."
                
        except Exception as e:
            return f"L·ªói truy v·∫•n DB: {str(e)}"
        
        formatted_context = ""
        for i, doc in enumerate(relevant_docs):
            source = doc.metadata.get("source_name", "T√†i li·ªáu n·ªôi b·ªô")
            content = doc.page_content.replace("\n", " ")
            formatted_context += f"[Ngu·ªìn {i+1}: {source}]\nN·ªôi dung: {content}\n\n"

        safe_history = chat_history.replace("{", "(").replace("}", ")")
        
        prompt = f"""B·∫°n l√† Tr·ª£ l√Ω HR c·ªßa Takagi Vi·ªát Nam.
        D·ªÆ LI·ªÜU: {formatted_context}
        L·ªäCH S·ª¨: {safe_history}
        C√ÇU H·ªéI: "{query}"
        TR·∫¢ L·ªúI:"""
        
        try:
            response = llm.invoke(prompt)
            return response.content
        except Exception as e:
            return f"L·ªói Gemini: {str(e)}"