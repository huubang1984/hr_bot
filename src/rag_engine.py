import os
import shutil
from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

class EnterpriseRAG:
    def __init__(self, persist_directory="./chroma_db"):
        self.persist_directory = persist_directory
        self.vector_store = None
        # L·∫•y Key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng (∆Øu ti√™n b·∫£o m·∫≠t)
        self.api_key = os.getenv("GOOGLE_API_KEY")

    def index_knowledge_base(self):
        """
        H√†m n√†y qu√©t th∆∞ m·ª•c 'data/' v√† c√°c th∆∞ m·ª•c con (HR, IT...) 
        ƒë·ªÉ n·∫°p ki·∫øn th·ª©c v√†o vector database.
        """
        # 1. D·ªçn d·∫πp b·ªô nh·ªõ c≈©
        if os.path.exists(self.persist_directory):
            try: shutil.rmtree(self.persist_directory)
            except: pass

        if not os.path.exists("data"):
            os.makedirs("data")
            return "‚ö†Ô∏è Th∆∞ m·ª•c 'data' ch∆∞a t·ªìn t·∫°i (ƒë√£ t·ª± ƒë·ªông t·∫°o). H√£y upload t√†i li·ªáu v√†o ƒë√≥."

        all_documents = []
        print("--- üöÄ B·∫ÆT ƒê·∫¶U QU√âT D·ªÆ LI·ªÜU ---")

        # 2. Qu√©t th√¥ng minh: H·ªó tr·ª£ c·∫£ file ·ªü g·ªëc v√† trong th∆∞ m·ª•c con (Ph√¢n lo·∫°i)
        # V√≠ d·ª•: data/HR/luong.txt -> category="HR"
        for root, dirs, files in os.walk("data"):
            category = os.path.basename(root) if root != "data" else "General"
            print(f"üìÇ ƒêang x·ª≠ l√Ω th∆∞ m·ª•c: {root} (Danh m·ª•c: {category})")
            
            # Load t·ª´ng lo·∫°i file trong th∆∞ m·ª•c hi·ªán t·∫°i
            docs = []
            try: docs.extend(DirectoryLoader(root, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'}, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.pdf", loader_cls=PyPDFLoader, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.docx", loader_cls=Docx2txtLoader, silent_errors=True).load())
            except: pass

            # G·∫Øn th·∫ª metadata (ƒë·ªÉ sau n√†y l·ªçc n·∫øu c·∫ßn)
            for doc in docs:
                doc.metadata["category"] = category
            
            all_documents.extend(docs)

        if not all_documents:
            return "‚ö†Ô∏è Ch∆∞a t√¨m th·∫•y t√†i li·ªáu n√†o trong th∆∞ m·ª•c data."

        # 3. C·∫Øt nh·ªè vƒÉn b·∫£n (Chunking)
        # chunk_size=2000: ƒê·ªß l·ªõn ƒë·ªÉ ch·ª©a tr·ªçn v·∫πn m·ªôt ƒëi·ªÅu lu·∫≠t
        # chunk_overlap=200: Gi·ªØ m·∫°ch vƒÉn
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
        texts = text_splitter.split_documents(all_documents)

        # 4. T·∫°o Vector Store (ChromaDB)
        if self.api_key:
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=self.api_key)
            try:
                self.vector_store = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=self.persist_directory)
                return f"‚úÖ TH√ÄNH C√îNG: ƒê√£ h·ªçc xong {len(all_documents)} t√†i li·ªáu (chia th√†nh {len(texts)} m·∫£nh ki·∫øn th·ª©c)."
            except Exception as e:
                return f"‚ùå L·ªñI INDEXING: {str(e)}"
        else:
            return "‚ùå L·ªñI: Ch∆∞a c√≥ GOOGLE_API_KEY."

    def retrieve_answer(self, query, chat_history="", category=None):
        """
        H√†m tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi kh·∫£ nƒÉng nh·ªõ ng·ªØ c·∫£nh v√† ƒë·ªãnh d·∫°ng Zalo.
        """
        if not self.api_key: return "L·ªói: Ch∆∞a c·∫•u h√¨nh API Key."
            
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=self.api_key)
        self.vector_store = Chroma(persist_directory=self.persist_directory, embedding_function=embeddings)
        
        # --- C·∫§U H√åNH AI "TH√îNG MINH & T·ª∞ NHI√äN" ---
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-pro", # D√πng b·∫£n Pro ƒë·ªÉ t∆∞ duy s√¢u
            google_api_key=self.api_key, 
            temperature=0.3,        # 0.3 gi√∫p vƒÉn phong m·ªÅm m·∫°i nh∆∞ng v·∫´n ch√≠nh x√°c
            max_output_tokens=8192, # Cho ph√©p tr·∫£ l·ªùi d√†i ƒë·∫ßy ƒë·ªß
            timeout=None,
            max_retries=2
        )
        
        # --- PROMPT CHUY√äN D·ª§NG CHO ZALO/MOBILE ---
        template = """B·∫°n l√† "Tr·ª£ l√Ω HR T·∫≠n t√¢m" c·ªßa C√¥ng ty Takagi Vi·ªát Nam.
        Nhi·ªám v·ª•: H·ªó tr·ª£ nh√¢n vi√™n gi·∫£i ƒë√°p th·∫Øc m·∫Øc v·ªÅ quy ƒë·ªãnh, ch√≠nh s√°ch, ph√∫c l·ª£i.

        L·ªäCH S·ª¨ TR√í CHUY·ªÜN (ƒê·ªÉ hi·ªÉu ng·ªØ c·∫£nh):
        {chat_history}

        D·ªÆ LI·ªÜU TRA C·ª®U (Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin n√†y):
        {context}

        C√ÇU H·ªéI C·ª¶A NH√ÇN VI√äN: "{question}"

        QUY T·∫ÆC TR·∫¢ L·ªúI QUAN TR·ªåNG (ZALO FRIENDLY):
        1. **ƒê·ªãnh d·∫°ng:** V√¨ hi·ªÉn th·ªã tr√™n ƒëi·ªán tho·∫°i (Zalo), TUY·ªÜT ƒê·ªêI KH√îNG D√ôNG B·∫¢NG (NO TABLE).
           - Thay v√†o ƒë√≥, h√£y d√πng danh s√°ch g·∫°ch ƒë·∫ßu d√≤ng ho·∫∑c chia ƒëo·∫°n nh·ªè.
           - V√≠ d·ª•: 
             * M·ª©c A: 1.000.000 ƒë
             * M·ª©c B: 2.000.000 ƒë
        2. **Th·∫•u c·∫£m:** B·∫Øt ƒë·∫ßu b·∫±ng gi·ªçng vƒÉn th√¢n thi·ªán, chia s·∫ª (ƒë·∫∑c bi·ªát v·ªõi c√°c v·∫•n ƒë·ªÅ ·ªëm ƒëau, thai s·∫£n, k·ª∑ lu·∫≠t).
        3. **Ch√≠nh x√°c:** Tr√≠ch d·∫´n s·ªë li·ªáu c·ª• th·ªÉ (ti·ªÅn, ng√†y th√°ng, %) v√† ghi ngu·ªìn vƒÉn b·∫£n ·ªü cu·ªëi.
        4. **Ng·ªØ c·∫£nh:** N·∫øu c√¢u h·ªèi kh√¥ng r√µ r√†ng (v√≠ d·ª• "c√≤n c√°i kia th√¨ sao?"), h√£y nh√¨n v√†o L·ªãch s·ª≠ tr√≤ chuy·ªán ƒë·ªÉ hi·ªÉu.

        PH·∫¢N H·ªíI C·ª¶A B·∫†N:"""
        
        QA_CHAIN_PROMPT = PromptTemplate(
            input_variables=["context", "question", "chat_history"],
            template=template
        )
        
        # C·∫•u h√¨nh t√¨m ki·∫øm
        search_kwargs = {"k": 6}
        # N·∫øu mu·ªën l·ªçc theo category (HR/IT), m·ªü comment d√≤ng d∆∞·ªõi:
        # if category: search_kwargs["filter"] = {"category": category}

        retriever = self.vector_store.as_retriever(search_kwargs=search_kwargs)
        
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={
                "prompt": QA_CHAIN_PROMPT,
                "memory": None
            }
        )
        
        # Th·ª±c thi v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
        try:
            result = qa_chain.invoke({"query": query, "chat_history": chat_history})
            return result["result"]
        except Exception as e:
            return f"Xin l·ªói, h·ªá th·ªëng ƒëang b·∫≠n. Vui l√≤ng th·ª≠ l·∫°i sau. (L·ªói: {str(e)})"