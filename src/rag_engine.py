import os
import time
# --- C·∫§U H√åNH GOOGLE CHAT (REST) ---
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["GLOG_minloglevel"] = "2"
import google.generativeai as genai

if os.getenv("GOOGLE_API_KEY"):
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"), transport="rest")

from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cohere import CohereEmbeddings
# --- TH∆Ø VI·ªÜN PINECONE M·ªöI ---
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec

class EnterpriseRAG:
    def __init__(self):
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.cohere_key = os.getenv("COHERE_API_KEY")
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        self.index_name = os.getenv("PINECONE_INDEX_NAME")
        
        # C·∫•u h√¨nh Cohere Embeddings (1024 dimensions)
        if self.cohere_key:
            self.embedding_model = CohereEmbeddings(
                cohere_api_key=self.cohere_key,
                model="embed-multilingual-v3.0"
            )
        else:
            self.embedding_model = None

    def index_knowledge_base(self):
        if not self.cohere_key: return "‚ùå L·ªói: Thi·∫øu COHERE_API_KEY."
        if not self.pinecone_api_key: return "‚ùå L·ªói: Thi·∫øu PINECONE_API_KEY."
        if not self.index_name: return "‚ùå L·ªói: Thi·∫øu PINECONE_INDEX_NAME."

        if not os.path.exists("data"):
            os.makedirs("data")
            return "Folder data created."
            
        all_documents = []
        print("--- üöÄ START INDEXING TO PINECONE CLOUD ---")
        
        # 1. Qu√©t t√†i li·ªáu
        for root, dirs, files in os.walk("data"):
            category = os.path.basename(root) if root != "data" else "General"
            docs = []
            try: docs.extend(DirectoryLoader(root, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'}, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.pdf", loader_cls=PyPDFLoader, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.docx", loader_cls=Docx2txtLoader, silent_errors=True).load())
            except: pass
            
            for doc in docs: 
                doc.metadata["category"] = category
                doc.metadata["source_name"] = os.path.basename(doc.metadata.get("source", ""))
            
            all_documents.extend(docs)

        if not all_documents: return "No documents found."
        
        # 2. C·∫Øt nh·ªè vƒÉn b·∫£n
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(all_documents)
        print(f"T·ªïng: {len(texts)} ƒëo·∫°n vƒÉn.")

        try:
            # 3. K·∫øt n·ªëi Pinecone v√† X√≥a d·ªØ li·ªáu c≈© (L√†m s·∫°ch Index)
            pc = Pinecone(api_key=self.pinecone_api_key)
            index = pc.Index(self.index_name)
            
            # X√≥a to√†n b·ªô vector c≈© ƒë·ªÉ n·∫°p m·ªõi (Gi·ªëng quy tr√¨nh Re-index c≈©)
            try:
                index.delete(delete_all=True)
                print("üóëÔ∏è ƒê√£ x√≥a d·ªØ li·ªáu c≈© tr√™n Cloud.")
                time.sleep(2) # ƒê·ª£i Pinecone x·ª≠ l√Ω x√≥a
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a (c√≥ th·ªÉ Index tr·ªëng): {e}")

            # 4. N·∫°p d·ªØ li·ªáu m·ªõi (Batching)
            vector_store = PineconeVectorStore(
                index_name=self.index_name,
                embedding=self.embedding_model,
                pinecone_api_key=self.pinecone_api_key
            )
            
            batch_size = 20
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i : i + batch_size]
                vector_store.add_documents(batch)
                print(f"‚òÅÔ∏è Pinecone Upload: Xong l√¥ {i//batch_size + 1}/{total_batches}")
                time.sleep(0.5) 
                
            return f"‚úÖ Th√†nh c√¥ng! ƒê√£ ƒë·∫©y {len(texts)} ƒëo·∫°n vƒÉn l√™n M√¢y (Pinecone)."
            
        except Exception as e:
            return f"‚ùå L·ªói Indexing Pinecone: {str(e)}"

    def retrieve_answer(self, query, chat_history="", category=None):
        if not self.api_key: return "L·ªói: Ch∆∞a c·∫•u h√¨nh Google API Key."
        if not self.index_name: return "L·ªói: Ch∆∞a c·∫•u h√¨nh Pinecone Index."
        
        vector_store = PineconeVectorStore(
            index_name=self.index_name,
            embedding=self.embedding_model,
            pinecone_api_key=self.pinecone_api_key
        )
        
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", 
            google_api_key=self.api_key, 
            temperature=0.3, 
            transport="rest",
            max_output_tokens=8192 
        )
        
        relevant_docs = []
        try:
            if category and category != "General":
                retriever = vector_store.as_retriever(
                    search_kwargs={"k": 5, "filter": {"category": category}}
                )
                relevant_docs = retriever.invoke(query)
            
            if not relevant_docs:
                retriever_all = vector_store.as_retriever(search_kwargs={"k": 5})
                relevant_docs = retriever_all.invoke(query)
                
            if not relevant_docs:
                return "D·∫°, hi·ªán t·∫°i em ch∆∞a t√¨m th·∫•y th√¥ng tin n√†y trong h·ªá th·ªëng d·ªØ li·ªáu."
                
        except Exception as e:
            return f"L·ªói truy v·∫•n Pinecone: {str(e)}"
        
        # --- S·ª¨A ƒê·ªîI 1: C√ÅCH FORMAT D·ªÆ LI·ªÜU ƒê·ªÇ AI ƒê·ªåC T√äN FILE ---
        formatted_context = ""
        for i, doc in enumerate(relevant_docs):
            source = doc.metadata.get("source_name", "T√†i li·ªáu n·ªôi b·ªô")
            content = doc.page_content.replace("\n", " ")
            # B·ªè ch·ªØ "Ngu·ªìn 1", ghi th·∫≥ng t√™n file ƒë·ªÉ AI tr√≠ch d·∫´n ƒë√∫ng
            formatted_context += f"--- T√ÄI LI·ªÜU THAM KH·∫¢O: {source} ---\nN·ªôi dung: {content}\n\n"

        safe_history = chat_history.replace("{", "(").replace("}", ")")
        
        # --- C·∫¨P NH·∫¨T PROMPT: Y√™u c·∫ßu tr·∫£ l·ªùi g·ªçn v√† ƒë·∫ßy ƒë·ªß ---
        prompt = f"""
        VAI TR√í:
        B·∫°n l√† Tr·ª£ l√Ω HR ·∫£o c·ªßa c√¥ng ty Takagi Vi·ªát Nam. T√™n b·∫°n l√† "Tr·ª£ l√Ω HR".
        B·∫°n x∆∞ng h√¥ l√† "em" v√† g·ªçi ng∆∞·ªùi d√πng l√† "anh/ch·ªã".
        T√≠nh c√°ch: T·∫≠n t√¢m, nh·∫π nh√†ng, chuy√™n nghi·ªáp nh∆∞ng g·∫ßn g≈©i.
        
        D·ªÆ LI·ªÜU TRA C·ª®U:
        {formatted_context}

        L·ªäCH S·ª¨ TR√í CHUY·ªÜN:
        {safe_history}

        C√ÇU H·ªéI M·ªöI: "{query}"

        Y√äU C·∫¶U TR·∫¢ L·ªúI (QUAN TR·ªåNG):
        1. **ƒê·ªô d√†i ph√π h·ª£p:** C√¢u tr·∫£ l·ªùi PH·∫¢I ng·∫Øn g·ªçn, s√∫c t√≠ch (t·ªëi ƒëa kho·∫£ng 1500 k√Ω t·ª±) ƒë·ªÉ hi·ªÉn th·ªã t·ªët tr√™n tin nh·∫Øn ƒëi·ªán tho·∫°i. KH√îNG vi·∫øt d√†i d√≤ng l√™ th√™.
        2. **C·∫•u tr√∫c:** S·ª≠ d·ª•ng g·∫°ch ƒë·∫ßu d√≤ng (-) cho c√°c √Ω ch√≠nh ƒë·ªÉ d·ªÖ ƒë·ªçc.
        3. **Tr√≠ch d·∫´n chu·∫©n:** Tuy·ªát ƒë·ªëi KH√îNG d√πng "Ngu·ªìn 1, Ngu·ªìn 2". H√£y ghi r√µ t√™n vƒÉn b·∫£n. 
           *V√≠ d·ª• ƒë√∫ng:* (Theo: Noi_quy_lao_dong_2024.pdf)
        4. **X·ª≠ l√Ω n·ªôi dung d√†i:** N·∫øu n·ªôi quy qu√° d√†i, h√£y t√≥m t·∫Øt c√°c ƒëi·ªÉm quan tr·ªçng nh·∫•t v√† n√≥i: "N·ªôi dung chi ti·∫øt anh/ch·ªã xem th√™m t·∫°i [T√™n t√†i li·ªáu] nh√© ·∫°."
	5. Cu·ªëi c√πng, ƒë·ªÅ xu·∫•t th√™m g·ª£i √Ω ho·∫∑c h·ªèi ng∆∞·ªùi d√πng c√≥ c·∫ßn th√™m h·ªó tr·ª£ n√†o kh√°c kh√¥ng, "em" s·∫µn s√†ng h·ªó tr·ª£ b·∫•t c·ª© l√∫c n√†o.

        B·∫ÆT ƒê·∫¶U TR·∫¢ L·ªúI:
        """
        
        try:
            response = llm.invoke(prompt)
            return response.content
        except Exception as e:
            return f"L·ªói Gemini: {str(e)}"