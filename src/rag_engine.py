mport os
import shutil
from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
# Thay ƒë·ªïi quan tr·ªçng: D√πng HuggingFace (Local) thay v√¨ Google API cho Embeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate

class EnterpriseRAG:
    def __init__(self, persist_directory="./chroma_db"):
        self.persist_directory = persist_directory
        self.vector_store = None
        self.api_key = os.getenv("GOOGLE_API_KEY")
        
        # S·ª¨ D·ª§NG LOCAL EMBEDDINGS (Mi·ªÖn ph√≠, ·ªîn ƒë·ªãnh)
        # Model 'all-MiniLM-L6-v2' r·∫•t nh·∫π v√† hi·ªáu qu·∫£
        self.embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

    def index_knowledge_base(self):
        # 1. D·ªçn d·∫πp DB c≈©
        if os.path.exists(self.persist_directory):
            try: shutil.rmtree(self.persist_directory)
            except: pass

        if not os.path.exists("data"):
            os.makedirs("data")
            return "Folder data created. Please upload files."
            
        all_documents = []
        print("--- üöÄ START INDEXING WITH LOCAL EMBEDDINGS ---")
        
        # 2. Qu√©t t√†i li·ªáu
        for root, dirs, files in os.walk("data"):
            category = os.path.basename(root) if root != "data" else "General"
            docs = []
            try: docs.extend(DirectoryLoader(root, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'}, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.pdf", loader_cls=PyPDFLoader, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.docx", loader_cls=Docx2txtLoader, silent_errors=True).load())
            except: pass
            
            for doc in docs: 
                doc.metadata["category"] = category
                file_name = os.path.basename(doc.metadata.get("source", ""))
                doc.metadata["source_name"] = file_name
            
            all_documents.extend(docs)

        if not all_documents: return "No documents found."
        
        # 3. C·∫Øt nh·ªè vƒÉn b·∫£n
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(all_documents)

        # 4. L∆∞u v√†o Vector DB (D√πng Local Embeddings)
        try:
            self.vector_store = Chroma.from_documents(
                documents=texts, 
                embedding=self.embedding_model, # D√πng model n·ªôi b·ªô
                persist_directory=self.persist_directory
            )
            return f"‚úÖ ƒê√£ h·ªçc xong {len(all_documents)} t√†i li·ªáu b·∫±ng Local Embeddings."
        except Exception as e:
            return f"‚ùå L·ªói Indexing: {str(e)}"

    def retrieve_answer(self, query, chat_history="", category=None):
        if not self.api_key: return "L·ªói: Ch∆∞a c·∫•u h√¨nh API Key cho Chat."
            
        # D√πng Local Embeddings ƒë·ªÉ t√¨m ki·∫øm
        self.vector_store = Chroma(
            persist_directory=self.persist_directory, 
            embedding_function=self.embedding_model
        )
        
        # D√πng Google Gemini ƒë·ªÉ TR·∫¢ L·ªúI (Ph·∫ßn n√†y v·∫´n c·∫ßn API Key, v√† n√≥ ƒëang ho·∫°t ƒë·ªông t·ªët)
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", 
            google_api_key=self.api_key, 
            temperature=0.1,
            max_output_tokens=8192
        )
        
        # --- T√åM KI·∫æM D·ªÆ LI·ªÜU ---
        search_kwargs = {"k": 5}
        if category: search_kwargs["filter"] = {"category": category}

        retriever = self.vector_store.as_retriever(search_kwargs=search_kwargs)
        relevant_docs = retriever.invoke(query)
        
        # X√¢y d·ª±ng Context
        formatted_context = ""
        for i, doc in enumerate(relevant_docs):
            source = doc.metadata.get("source_name", "T√†i li·ªáu n·ªôi b·ªô")
            content = doc.page_content.replace("\n", " ")
            formatted_context += f"[Ngu·ªìn {i+1}: {source}]\nN·ªôi dung: {content}\n\n"

        safe_history = chat_history.replace("{", "(").replace("}", ")")
        
        # --- PROMPT K·ª∂ LU·∫¨T TH√âP ---
        prompt = f"""B·∫°n l√† Tr·ª£ l√Ω HR chuy√™n nghi·ªáp v√† t·∫≠n t√¢m c·ªßa Takagi Vi·ªát Nam.
        
        NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.
        
        QUY T·∫ÆC B·∫ÆT BU·ªòC (TU√ÇN TH·ª¶ TUY·ªÜT ƒê·ªêI):
        1. **CH·ªà S·ª¨ D·ª§NG** th√¥ng tin trong ph·∫ßn "D·ªÆ LI·ªÜU TRA C·ª®U" b√™n d∆∞·ªõi.
        2. **KH√îNG** ƒë∆∞·ª£c t·ª± b·ªãa ra ki·∫øn th·ª©c b√™n ngo√†i (n·∫øu kh√¥ng c√≥ trong t√†i li·ªáu, h√£y n√≥i: "Xin l·ªói, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu n·ªôi b·ªô").
        3. **TR√çCH D·∫™N NGU·ªíN:** Cu·ªëi m·ªói √Ω ho·∫∑c cu·ªëi c√¢u tr·∫£ l·ªùi, PH·∫¢I ghi r√µ th√¥ng tin l·∫•y t·ª´ ƒë√¢u.
           - V√≠ d·ª•: "...theo quy ƒë·ªãnh m·ªõi (Ngu·ªìn: Noi_quy_2025.pdf)".
        4. Tr√¨nh b√†y g·∫°ch ƒë·∫ßu d√≤ng, ng·∫Øn g·ªçn, d·ªÖ ƒë·ªçc tr√™n ƒëi·ªán tho·∫°i.

QUY T·∫ÆC TR·∫¢ L·ªúI (ZALO):
1. KH√îNG D√ôNG B·∫¢NG (No Tables). D√πng g·∫°ch ƒë·∫ßu d√≤ng.
2. Th√¢n thi·ªán, ch√≠nh x√°c s·ªë li·ªáu.
3. K·∫øt h·ª£p l·ªãch s·ª≠ chat ƒë·ªÉ hi·ªÉu c√¢u h·ªèi c·ªôc l·ªëc.

        ---
        L·ªäCH S·ª¨ CHAT:
        {safe_history}
        ---
        D·ªÆ LI·ªÜU TRA C·ª®U (CONTEXT):
        {context_text}
        ---
        C√ÇU H·ªéI C·ª¶A NH√ÇN VI√äN: "{query}"
        
        TR·∫¢ L·ªúI:"""
        
        # G·ªçi th·∫≥ng LLM (B·ªè qua Chain ph·ª©c t·∫°p ƒë·ªÉ ki·ªÉm so√°t t·ªët h∆°n)
        try:
            response = llm.invoke(prompt)
            return response.content
        except Exception as e:
            return f"L·ªói x·ª≠ l√Ω: {str(e)}"
