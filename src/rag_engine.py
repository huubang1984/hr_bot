import os
import shutil
from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

class EnterpriseRAG:
    def __init__(self, persist_directory="./chroma_db"):
        self.persist_directory = persist_directory
        self.vector_store = None
        self.api_key = os.getenv("GOOGLE_API_KEY")

    def index_knowledge_base(self):
        # ... (Gi·ªØ nguy√™n ph·∫ßn Indexing kh√¥ng ƒë·ªïi) ...
        # (ƒê·ªÉ ng·∫Øn g·ªçn, t√¥i kh√¥ng paste l·∫°i ph·∫ßn Indexing d√†i d√≤ng ·ªü ƒë√¢y v√¨ n√≥ v·∫´n ho·∫°t ƒë·ªông t·ªët)
        if os.path.exists(self.persist_directory):
            try: shutil.rmtree(self.persist_directory)
            except: pass

        if not os.path.exists("data"):
            os.makedirs("data")
            return "‚ö†Ô∏è Th∆∞ m·ª•c 'data' ch∆∞a t·ªìn t·∫°i."

        all_documents = []
        print("--- üöÄ B·∫ÆT ƒê·∫¶U QU√âT D·ªÆ LI·ªÜU ---")
        for root, dirs, files in os.walk("data"):
            category = os.path.basename(root) if root != "data" else "General"
            docs = []
            try: docs.extend(DirectoryLoader(root, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'}, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.pdf", loader_cls=PyPDFLoader, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.docx", loader_cls=Docx2txtLoader, silent_errors=True).load())
            except: pass
            for doc in docs: doc.metadata["category"] = category
            all_documents.extend(docs)

        if not all_documents: return "‚ö†Ô∏è Ch∆∞a t√¨m th·∫•y t√†i li·ªáu n√†o."
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
        texts = text_splitter.split_documents(all_documents)

        if self.api_key:
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=self.api_key)
            try:
                self.vector_store = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=self.persist_directory)
                return f"‚úÖ TH√ÄNH C√îNG: ƒê√£ h·ªçc xong {len(all_documents)} t√†i li·ªáu."
            except Exception as e: return f"‚ùå L·ªñI INDEXING: {str(e)}"
        else: return "‚ùå L·ªñI: Ch∆∞a c√≥ GOOGLE_API_KEY."

    def retrieve_answer(self, query, chat_history="", category=None):
        """
        H√†m tr·∫£ l·ªùi c√¢u h·ªèi - PHI√äN B·∫¢N S·ª¨A L·ªñI INPUT KEYS
        """
        if not self.api_key: return "L·ªói: Ch∆∞a c·∫•u h√¨nh API Key."
            
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=self.api_key)
        self.vector_store = Chroma(persist_directory=self.persist_directory, embedding_function=embeddings)
        
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-pro", 
            google_api_key=self.api_key, 
            temperature=0.3,        
            max_output_tokens=8192,
            timeout=None,
            max_retries=2
        )
        
        # --- FIX L·ªñI ·ªû ƒê√ÇY ---
        # 1. X·ª≠ l√Ω chat_history b·∫±ng Python thu·∫ßn t√∫y ƒë·ªÉ tr√°nh l·ªói k√Ω t·ª± ƒë·∫∑c bi·ªát
        safe_history = chat_history.replace("{", "(").replace("}", ")")
        
        # 2. B∆°m th·∫≥ng history v√†o string (F-string injection)
        # L∆∞u √Ω: Ph·∫£i d√πng {{context}} v√† {{question}} (2 d·∫•u ngo·∫∑c) ƒë·ªÉ gi·ªØ l·∫°i bi·∫øn cho LangChain
        template = f"""B·∫°n l√† "Tr·ª£ l√Ω HR T·∫≠n t√¢m" c·ªßa C√¥ng ty Takagi Vi·ªát Nam.
        
        L·ªäCH S·ª¨ TR√í CHUY·ªÜN:
        {safe_history}

        D·ªÆ LI·ªÜU TRA C·ª®U:
        {{context}}

        C√ÇU H·ªéI C·ª¶A NH√ÇN VI√äN: "{{question}}"

        QUY T·∫ÆC TR·∫¢ L·ªúI (ZALO):
        1. KH√îNG D√ôNG B·∫¢NG (No Tables). D√πng g·∫°ch ƒë·∫ßu d√≤ng.
        2. Th√¢n thi·ªán, ch√≠nh x√°c s·ªë li·ªáu.
        3. K·∫øt h·ª£p l·ªãch s·ª≠ chat ƒë·ªÉ hi·ªÉu c√¢u h·ªèi c·ªôc l·ªëc.

        PH·∫¢N H·ªíI:"""
        
        # 3. Khai b√°o PromptTemplate CH·ªà C√íN 2 bi·∫øn (context, question)
        QA_CHAIN_PROMPT = PromptTemplate(
            input_variables=["context", "question"], # ƒê√£ b·ªè chat_history ra kh·ªèi danh s√°ch n√†y
            template=template
        )
        
        search_kwargs = {"k": 6}
        if category: search_kwargs["filter"] = {"category": category}

        retriever = self.vector_store.as_retriever(search_kwargs=search_kwargs)
        
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={
                "prompt": QA_CHAIN_PROMPT
            }
        )
        
        try:
            # 4. Invoke ch·ªâ c·∫ßn query (LangChain s·∫Ω t·ª± ƒëi·ªÅn v√†o {{question}})
            result = qa_chain.invoke({"query": query})
            return result["result"]
        except Exception as e:
            return f"Xin l·ªói, h·ªá th·ªëng ƒëang b·∫≠n. (L·ªói chi ti·∫øt: {str(e)})"