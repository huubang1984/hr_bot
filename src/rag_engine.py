import os
import time
# --- C·∫§U H√åNH GOOGLE CHAT (REST) ---
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["GLOG_minloglevel"] = "2"
import google.generativeai as genai

if os.getenv("GOOGLE_API_KEY"):
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"), transport="rest")

from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cohere import CohereEmbeddings
# --- TH∆Ø VI·ªÜN PINECONE M·ªöI ---
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec

class EnterpriseRAG:
    def __init__(self):
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.cohere_key = os.getenv("COHERE_API_KEY")
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        self.index_name = os.getenv("PINECONE_INDEX_NAME")
        
        # C·∫•u h√¨nh Cohere Embeddings (1024 dimensions)
        if self.cohere_key:
            self.embedding_model = CohereEmbeddings(
                cohere_api_key=self.cohere_key,
                model="embed-multilingual-v3.0"
            )
        else:
            self.embedding_model = None

    def index_knowledge_base(self):
        if not self.cohere_key: return "‚ùå L·ªói: Thi·∫øu COHERE_API_KEY."
        if not self.pinecone_api_key: return "‚ùå L·ªói: Thi·∫øu PINECONE_API_KEY."
        if not self.index_name: return "‚ùå L·ªói: Thi·∫øu PINECONE_INDEX_NAME."

        if not os.path.exists("data"):
            os.makedirs("data")
            return "Folder data created."
            
        all_documents = []
        print("--- üöÄ START INDEXING TO PINECONE CLOUD ---")
        
        # 1. Qu√©t t√†i li·ªáu
        for root, dirs, files in os.walk("data"):
            category = os.path.basename(root) if root != "data" else "General"
            docs = []
            try: docs.extend(DirectoryLoader(root, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'}, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.pdf", loader_cls=PyPDFLoader, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.docx", loader_cls=Docx2txtLoader, silent_errors=True).load())
            except: pass
            
            for doc in docs: 
                doc.metadata["category"] = category
                doc.metadata["source_name"] = os.path.basename(doc.metadata.get("source", ""))
            
            all_documents.extend(docs)

        if not all_documents: return "No documents found."
        
        # 2. C·∫Øt nh·ªè vƒÉn b·∫£n
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(all_documents)
        print(f"T·ªïng: {len(texts)} ƒëo·∫°n vƒÉn.")

        try:
            # 3. K·∫øt n·ªëi Pinecone v√† X√≥a d·ªØ li·ªáu c≈© (L√†m s·∫°ch Index)
            pc = Pinecone(api_key=self.pinecone_api_key)
            index = pc.Index(self.index_name)
            
            # X√≥a to√†n b·ªô vector c≈© ƒë·ªÉ n·∫°p m·ªõi (Gi·ªëng quy tr√¨nh Re-index c≈©)
            try:
                index.delete(delete_all=True)
                print("üóëÔ∏è ƒê√£ x√≥a d·ªØ li·ªáu c≈© tr√™n Cloud.")
                time.sleep(2) # ƒê·ª£i Pinecone x·ª≠ l√Ω x√≥a
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a (c√≥ th·ªÉ Index tr·ªëng): {e}")

            # 4. N·∫°p d·ªØ li·ªáu m·ªõi (Batching)
            vector_store = PineconeVectorStore(
                index_name=self.index_name,
                embedding=self.embedding_model,
                pinecone_api_key=self.pinecone_api_key
            )
            
            batch_size = 20
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i : i + batch_size]
                vector_store.add_documents(batch)
                print(f"‚òÅÔ∏è Pinecone Upload: Xong l√¥ {i//batch_size + 1}/{total_batches}")
                time.sleep(0.5) 
                
            return f"‚úÖ Th√†nh c√¥ng! ƒê√£ ƒë·∫©y {len(texts)} ƒëo·∫°n vƒÉn l√™n M√¢y (Pinecone)."
            
        except Exception as e:
            return f"‚ùå L·ªói Indexing Pinecone: {str(e)}"

    def retrieve_answer(self, query, chat_history="", category=None):
        if not self.api_key: return "L·ªói: Ch∆∞a c·∫•u h√¨nh Google API Key."
        if not self.index_name: return "L·ªói: Ch∆∞a c·∫•u h√¨nh Pinecone Index."
        
        # K·∫øt n·ªëi Vector Store t·ª´ Cloud
        vector_store = PineconeVectorStore(
            index_name=self.index_name,
            embedding=self.embedding_model,
            pinecone_api_key=self.pinecone_api_key
        )
        
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", 
            google_api_key=self.api_key, 
            temperature=0.3, 
            transport="rest"
        )
        
        relevant_docs = []
        try:
            # 1. T√¨m ki·∫øm (Logic c≈©)
            if category and category != "General":
                retriever = vector_store.as_retriever(
                    search_kwargs={"k": 5, "filter": {"category": category}}
                )
                relevant_docs = retriever.invoke(query)
            
            if not relevant_docs:
                retriever_all = vector_store.as_retriever(search_kwargs={"k": 5})
                relevant_docs = retriever_all.invoke(query)
                
            if not relevant_docs:
                return "D·∫°, hi·ªán t·∫°i em ch∆∞a t√¨m th·∫•y th√¥ng tin n√†y trong h·ªá th·ªëng d·ªØ li·ªáu."
                
        except Exception as e:
            return f"L·ªói truy v·∫•n Pinecone: {str(e)}"
        
        formatted_context = ""
        for i, doc in enumerate(relevant_docs):
            source = doc.metadata.get("source_name", "T√†i li·ªáu n·ªôi b·ªô")
            content = doc.page_content.replace("\n", " ")
            formatted_context += f"[Ngu·ªìn {i+1}: {source}]\nN·ªôi dung: {content}\n\n"

        safe_history = chat_history.replace("{", "(").replace("}", ")")
        
        prompt = f"""
        VAI TR√í:
        B·∫°n l√† Tr·ª£ l√Ω HR ·∫£o c·ªßa c√¥ng ty Takagi Vi·ªát Nam. T√™n b·∫°n l√† "Tr·ª£ l√Ω HR".
        B·∫°n x∆∞ng h√¥ l√† "em" v√† g·ªçi ng∆∞·ªùi d√πng l√† "anh/ch·ªã".
        T√≠nh c√°ch: T·∫≠n t√¢m, nh·∫π nh√†ng, chuy√™n nghi·ªáp nh∆∞ng g·∫ßn g≈©i.

        NHI·ªÜM V·ª§:
        Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa nh√¢n vi√™n d·ª±a tr√™n D·ªÆ LI·ªÜU ƒê∆Ø·ª¢C CUNG C·∫§P d∆∞·ªõi ƒë√¢y.

        D·ªÆ LI·ªÜU TRA C·ª®U:
        {formatted_context}

        L·ªäCH S·ª¨ TR√í CHUY·ªÜN:
        {safe_history}

        C√ÇU H·ªéI M·ªöI: "{query}"

        Y√äU C·∫¶U TR·∫¢ L·ªúI:
        1. **Trung th·ª±c v·ªõi d·ªØ li·ªáu:** Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin trong ph·∫ßn "D·ªÆ LI·ªÜU TRA C·ª®U".
        2. **X·ª≠ l√Ω khi thi·∫øu tin:** N·∫øu d·ªØ li·ªáu kh√¥ng ch·ª©a c√¢u tr·∫£ l·ªùi, h√£y n√≥i: "D·∫°, v·∫•n ƒë·ªÅ n√†y em t√¨m trong t√†i li·ªáu n·ªôi b·ªô ch∆∞a th·∫•y ƒë·ªÅ c·∫≠p. Anh/ch·ªã li√™n h·ªá tr·ª±c ti·∫øp ph√≤ng Nh√¢n s·ª± ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ ch√≠nh x√°c nh·∫•t nh√© ·∫°."
        3. **Tr√≠ch d·∫´n ngu·ªìn:** Cu·ªëi c√¢u tr·∫£ l·ªùi, h√£y ghi ch√∫ ngu·ªìn t√†i li·ªáu tham kh·∫£o. V√≠ d·ª•: (Theo: Noi_quy_cong_ty.pdf).
        4. **Phong c√°ch:** Lu√¥n ƒë∆∞a ra m·ªôt l·ªùi khuy√™n, ƒë·ªÅ xu·∫•t ho·∫∑c h√†nh ƒë·ªông ti·∫øp theo ·ªü cu·ªëi c√¢u tr·∫£ l·ªùi.

        B·∫ÆT ƒê·∫¶U TR·∫¢ L·ªúI:
        """
        
        try:
            response = llm.invoke(prompt)
            return response.content
        except Exception as e:
            return f"L·ªói Gemini: {str(e)}"