import os
import shutil
import time

# --- C·∫§U H√åNH GOOGLE CHAT (REST) ---
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["GLOG_minloglevel"] = "2"
import google.generativeai as genai

# C·∫•u h√¨nh Google GenAI
if os.getenv("GOOGLE_API_KEY"):
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"), transport="rest")

from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
# S·ª¨ D·ª§NG COHERE (·ªîn ƒë·ªãnh nh·∫•t cho g√≥i Free)
from langchain_cohere import CohereEmbeddings

class EnterpriseRAG:
    def __init__(self, persist_directory="./chroma_db"):
        self.persist_directory = persist_directory
        self.vector_store = None
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.cohere_key = os.getenv("COHERE_API_KEY")
        
        # C·∫•u h√¨nh Cohere Embeddings
        if self.cohere_key:
            self.embedding_model = CohereEmbeddings(
                cohere_api_key=self.cohere_key,
                model="embed-multilingual-v3.0"
            )
        else:
            self.embedding_model = None

def index_knowledge_base(self):
        if not self.cohere_key: return "‚ùå L·ªói: Thi·∫øu COHERE_API_KEY."

        # --- ƒêO·∫†N CODE M·ªöI QUAN TR·ªåNG: GI·∫¢I PH√ìNG DB C≈® ---
        # Ng·∫Øt k·∫øt n·ªëi Chroma hi·ªán t·∫°i ƒë·ªÉ tr√°nh l·ªói "Readonly database"
        self.vector_store = None
        import gc
        gc.collect()
        # --------------------------------------------------

        # 1. D·ªçn d·∫πp DB c≈©
        if os.path.exists(self.persist_directory):
            try: shutil.rmtree(self.persist_directory)
            except: pass # N·∫øu v·∫´n kh√¥ng x√≥a ƒë∆∞·ª£c th√¨ b·ªè qua, Chroma s·∫Ω t·ª± x·ª≠ l√Ω ghi ƒë√®

        if not os.path.exists("data"):
            os.makedirs("data")
            return "Folder data created."
            
        all_documents = []
        print("--- üöÄ START INDEXING WITH COHERE ---")
        
        # 2. Qu√©t t√†i li·ªáu
        for root, dirs, files in os.walk("data"):
            category = os.path.basename(root) if root != "data" else "General"
            docs = []
            try: docs.extend(DirectoryLoader(root, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'}, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.pdf", loader_cls=PyPDFLoader, silent_errors=True).load())
            except: pass
            try: docs.extend(DirectoryLoader(root, glob="*.docx", loader_cls=Docx2txtLoader, silent_errors=True).load())
            except: pass
            
            for doc in docs: 
                doc.metadata["category"] = category
                doc.metadata["source_name"] = os.path.basename(doc.metadata.get("source", ""))
            
            all_documents.extend(docs)

        if not all_documents: return "No documents found."
        
        # 3. C·∫Øt nh·ªè vƒÉn b·∫£n
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(all_documents)
        print(f"T·ªïng: {len(texts)} ƒëo·∫°n vƒÉn.")

        # 4. L∆∞u v√†o DB (Batching)
        try:
            self.vector_store = Chroma(
                embedding_function=self.embedding_model,
                persist_directory=self.persist_directory
            )
            
            batch_size = 20
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i : i + batch_size]
                self.vector_store.add_documents(batch)
                print(f"‚úÖ Cohere: Xong l√¥ {i//batch_size + 1}/{total_batches}")
                time.sleep(0.5) 
                
            return f"‚úÖ Th√†nh c√¥ng! ƒê√£ h·ªçc xong {len(all_documents)} t√†i li·ªáu (Cohere Enterprise)."
            
        except Exception as e:
            return f"‚ùå L·ªói Indexing Cohere: {str(e)}"

    def retrieve_answer(self, query, chat_history="", category=None):
        if not self.api_key: return "L·ªói: Ch∆∞a c·∫•u h√¨nh Google API Key."
        if not self.embedding_model: return "L·ªói: Ch∆∞a c·∫•u h√¨nh Cohere API Key."
            
        self.vector_store = Chroma(
            persist_directory=self.persist_directory, 
            embedding_function=self.embedding_model
        )
        
        # Model Chat (Google Gemini) - TƒÉng temp l√™n x√≠u cho t·ª± nhi√™n
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", 
            google_api_key=self.api_key, 
            temperature=0.3, 
            transport="rest"
        )
        
        # --- LOGIC T√åM KI·∫æM TH√îNG MINH H∆†N ---
        relevant_docs = []
        try:
            # 1. Th·ª≠ t√¨m v·ªõi category (n·∫øu c√≥)
            if category and category != "General":
                retriever = self.vector_store.as_retriever(
                    search_kwargs={"k": 5, "filter": {"category": category}}
                )
                relevant_docs = retriever.invoke(query)
            
            # 2. N·∫øu kh√¥ng t√¨m th·∫•y ho·∫∑c kh√¥ng c√≥ category, t√¨m to√†n b·ªô DB
            if not relevant_docs:
                retriever_all = self.vector_store.as_retriever(search_kwargs={"k": 5})
                relevant_docs = retriever_all.invoke(query)
                
            # N·∫øu v·∫´n kh√¥ng c√≥ -> H·ªá th·ªëng th·ª±c s·ª± r·ªóng ho·∫∑c ch∆∞a Re-index
            if not relevant_docs:
                return "D·∫°, hi·ªán t·∫°i em ch∆∞a t√¨m th·∫•y th√¥ng tin n√†y trong h·ªá th·ªëng d·ªØ li·ªáu. Anh/ch·ªã ki·ªÉm tra l·∫°i xem ƒë√£ c·∫≠p nh·∫≠t t√†i li·ªáu (Re-index) ch∆∞a ·∫°?"
                
        except Exception as e:
            return f"L·ªói truy v·∫•n DB: {str(e)}"
        
        # X√¢y d·ª±ng Context
        formatted_context = ""
        unique_sources = set()
        for i, doc in enumerate(relevant_docs):
            source = doc.metadata.get("source_name", "T√†i li·ªáu n·ªôi b·ªô")
            unique_sources.add(source)
            content = doc.page_content.replace("\n", " ")
            formatted_context += f"[Ngu·ªìn {i+1}: {source}]\nN·ªôi dung: {content}\n\n"

        safe_history = chat_history.replace("{", "(").replace("}", ")")
        
        # --- PROMPT M·ªöI (CHU·∫®N PERSONA & T·∫¨N T√ÇM) ---
        prompt = f"""
        VAI TR√í:
        B·∫°n l√† Tr·ª£ l√Ω HR ·∫£o c·ªßa c√¥ng ty Takagi Vi·ªát Nam. T√™n b·∫°n l√† "Tr·ª£ l√Ω HR".
        B·∫°n x∆∞ng h√¥ l√† "em" v√† g·ªçi ng∆∞·ªùi d√πng l√† "anh/ch·ªã".
        T√≠nh c√°ch: T·∫≠n t√¢m, nh·∫π nh√†ng, chuy√™n nghi·ªáp nh∆∞ng g·∫ßn g≈©i, lu√¥n mu·ªën gi√∫p ƒë·ª° nh√¢n vi√™n.

        NHI·ªÜM V·ª§:
        Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa nh√¢n vi√™n d·ª±a tr√™n D·ªÆ LI·ªÜU ƒê∆Ø·ª¢C CUNG C·∫§P d∆∞·ªõi ƒë√¢y.

        D·ªÆ LI·ªÜU TRA C·ª®U:
        {formatted_context}

        L·ªäCH S·ª¨ TR√í CHUY·ªÜN:
        {safe_history}

        C√ÇU H·ªéI M·ªöI: "{query}"

        Y√äU C·∫¶U TR·∫¢ L·ªúI:
        1. **Trung th·ª±c v·ªõi d·ªØ li·ªáu:** Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin trong ph·∫ßn "D·ªÆ LI·ªÜU TRA C·ª®U".
        2. **X·ª≠ l√Ω khi thi·∫øu tin:** N·∫øu d·ªØ li·ªáu kh√¥ng ch·ª©a c√¢u tr·∫£ l·ªùi, h√£y n√≥i: "D·∫°, v·∫•n ƒë·ªÅ n√†y em t√¨m trong t√†i li·ªáu n·ªôi b·ªô ch∆∞a th·∫•y ƒë·ªÅ c·∫≠p. Anh/ch·ªã li√™n h·ªá tr·ª±c ti·∫øp ph√≤ng Nh√¢n s·ª± ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ ch√≠nh x√°c nh·∫•t nh√© ·∫°."
        3. **Tr√≠ch d·∫´n ngu·ªìn:** Cu·ªëi c√¢u tr·∫£ l·ªùi, h√£y ghi ch√∫ ngu·ªìn t√†i li·ªáu tham kh·∫£o. V√≠ d·ª•: (Theo: Noi_quy_cong_ty.pdf).
        4. **Phong c√°ch:** - B·∫Øt ƒë·∫ßu b·∫±ng l·ªùi ch√†o nh·∫π nh√†ng n·∫øu c·∫ßn.
           - Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu.
           - **QUAN TR·ªåNG:** Lu√¥n ƒë∆∞a ra m·ªôt l·ªùi khuy√™n, ƒë·ªÅ xu·∫•t ho·∫∑c h√†nh ƒë·ªông ti·∫øp theo ·ªü cu·ªëi c√¢u tr·∫£ l·ªùi ƒë·ªÉ h·ªó tr·ª£ nh√¢n vi√™n t·ªët nh·∫•t (V√≠ d·ª•: "Anh/ch·ªã nh·ªõ n·ªôp ƒë∆°n tr∆∞·ªõc ng√†y 5 nh√©", "N·∫øu c·∫ßn m·∫´u ƒë∆°n, anh/ch·ªã b·∫£o em nha").

        B·∫ÆT ƒê·∫¶U TR·∫¢ L·ªúI:
        """
        
        try:
            response = llm.invoke(prompt)
            return response.content
        except Exception as e:
            return f"L·ªói Gemini: {str(e)}"